\section{Related Work} \label{sec:related}
General purpose GPUs become important processing engines for computationally intensive workloads due to their highly parallel computing architectures.
CUBLAS provides many solutions for low-level linear algebra operations supported by Nvidia \cite{cublas}. 
CULA consists of commercial hybrid GPU accelerated linear algebra routines \cite{cula}.
MAGMA aims to achieve high performance and portability across a wide range of multi-core architectures and hybrid systems respectively\cite{magma}.

The first SVD algorithm with CUDA programming is implemented by Sheetal et al.\cite{09IPDPSQR}. With parallelized QR iteration algorithm using CUBLAS library on GPU, they achieved a speedup of up to 8 over the Intel MKL QR implementation.
Liu et al. use a divide-and-conquer approach to solve SVD on a heterogeneous CPU-GPU system \cite{13CFDC}.
It is almost 7 times faster than CULA QR algorithm executing on the same device M2070, and up to 33 times faster than LAPACK.
Vedran\cite{14arxivjacobi} presents a hierarchically blocked one-sided Jacobi algorithm for the singular value decomposition on both single and multiple GPU architectures. Even with full optimizations and a high speedup compared to the same algorithm on CPU, the execution time is still more than that of Sheetal's QR implementation.

Drineas et al. \cite{99clustering} provide a clustered SVD algorithm for large matrices. The algorithm divides a set of $n$ points into $k$ clusters, where $k$ is much less than $n$ on CPU.
It is an approximation algorithm to obtain only one subset of singular values
and vectors. Our BT algorithm can compute the complete SVD, thus is not
directly compared with \cite{99clustering}. 

